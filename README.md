# ðŸ¦™ fauxllama

**fauxllama** is a drop-in Ollama-compatible API server designed to simulate a local `Ollama` instance for GitHub Copilot Chat integrations. It allows developers to bring their own model backend including Azure OpenAI, RAG pipelines, or finetuned models and seamlessly use them in **Visual Studio Code** through the `byok.ollamaEndpoint` setting.

> TL;DR: fauxllama tricks VS Code into thinking it's talking to Ollama - but it's actually your own custom backend.

---

## Features

- **Ollama-compatible API** for Copilot Chat BYOK (`byok.ollamaEndpoint`)
-  **API Key Management** with per-user access control
-  **Chat Logging** for dataset generation and training
-  **Streaming Responses** via Azure OpenAI-compatible completions
-  **Model Metadata Simulation** (e.g., family, quantization, digest)
-  **Admin Panel** with basic auth to manage API keys
-  PostgreSQL-backed, with Flask-Migrate for schema control
-  Ready to deploy via `docker-compose`

---

## Use Case

The main use case is enabling **Copilot Chat** in VS Code to interact with your **own model infrastructure**, such as:

- Azure OpenAI deployments (GPT-4, GPT-3.5)
- Fine-tuned or private-hosted LLMs
- RAG pipelines or local inference stacks

By configuring VS Code like so:

```json
"github.copilot.chat.byok.ollamaEndpoint": "http://localhost:11434/YOUR_API_KEY"
```

---

## Setup Instructions

1. Clone the repo
```bash
git clone https://github.com/ManosMrgk/fauxllama.git
cd fauxllama
```
2. Create your environment config
```bash
cp .env.example .env
```
3. Start with Docker Compose
```bash
docker-compose up --build
```

Once running, the API will be live at:
http://localhost:11434

---

## API Key Management
To create and manage API keys:

Visit the admin panel at:
http://localhost:11434/

Login using your ADMIN_PASSWORD from .env

Create a new API key per user

Each key maps to a user and is required for accessing chat endpoints.

---

## API Overview
All requests must include a valid API_KEY in the URL path:

```bash
POST /<API_KEY>/v1/chat/completions
```

---

## VS Code Integration
To use fauxllama as your GitHub Copilot Chat backend:

1. Open VS Code settings.json

2. Add:

    ```json
    "github.copilot.chat.byok.ollamaEndpoint": "http://localhost:11434/YOUR_API_KEY"
    ```
3. Restart VS Code

Now, Copilot Chat will route requests through fauxllama.

---
**fauxllama** - Not a real llama. Just your LLM in disguise. ðŸ¦™
