# ğŸ¦™ fauxllama

**fauxllama** is a drop-in Ollama-compatible API server designed to simulate a local `Ollama` instance for GitHub Copilot Chat integrations. It allows developers to bring their own model backend â€” including Azure OpenAI, RAG pipelines, or finetuned models â€” and seamlessly use them in **Visual Studio Code** through the `byok.ollamaEndpoint` setting.

> TL;DR: fauxllama tricks VS Code into thinking it's talking to Ollama â€” but it's actually your own custom backend.

---

## âœ¨ Features

- âœ… **Ollama-compatible API** for Copilot Chat BYOK (`byok.ollamaEndpoint`)
- ğŸ” **API Key Management** with per-user access control
- ğŸ’¬ **Chat Logging** for dataset generation and training
- ğŸ” **Streaming Responses** via Azure OpenAI-compatible completions
- ğŸ§  **Model Metadata Simulation** (e.g., family, quantization, digest)
- ğŸ“ˆ **Admin Panel** with basic auth to manage API keys
- ğŸ˜ PostgreSQL-backed, with Flask-Migrate for schema control
- ğŸ³ Ready to deploy via `docker-compose`

---

## ğŸ“¦ Project Structure
```
fauxllama/
â”œâ”€â”€ server.py                      # Main Flask API server (entry point)
â”œâ”€â”€ requirements.txt               # Python dependencies
â”œâ”€â”€ Dockerfile                    # Backend container
â”œâ”€â”€ docker-compose.yml             # App + PostgreSQL orchestration
â”œâ”€â”€ .env.example                  # Environment variable template
â”œâ”€â”€ migrations/                   # Flask-Migrate migration scripts

â”œâ”€â”€ app/                         # Application package
â”‚   â”œâ”€â”€ __init__.py              # Flask app factory & extensions initialization
â”‚   â”œâ”€â”€ models.py                # SQLAlchemy models
â”‚   â”œâ”€â”€ views/                   # Flask route handlers
â”‚   â”‚   â”œâ”€â”€ __init__.py
â”‚   â”‚   â”œâ”€â”€ api.py               # API endpoints
â”‚   â”‚   â””â”€â”€ admin.py             # Flask-Admin setup & views
â”‚   â”œâ”€â”€ utils/                   # Helper utilities
â”‚   â”‚   â”œâ”€â”€ __init__.py
â”‚   â”‚   â”œâ”€â”€ auth.py              # Auth (API key + HTTP Basic Auth)
â”‚   â”‚   â”œâ”€â”€ azure_openai.py      # Azure OpenAI integration helpers
â”‚   â”‚   â”œâ”€â”€ db_helpers.py        # DB helper functions (logging etc.)
â”‚   â”‚   â””â”€â”€ limiter.py           # Rate limiting config
â”‚   â””â”€â”€ extensions.py            # Flask extensions (db, migrate, limiter, admin)
â”‚
â””â”€â”€ README.md                    
```

---

## ğŸš€ Use Case

The main use case is enabling **Copilot Chat** in VS Code to interact with your **own model infrastructure**, such as:

- Azure OpenAI deployments (GPT-4, GPT-3.5)
- Fine-tuned or private-hosted LLMs
- RAG pipelines or local inference stacks

By configuring VS Code like so:

```json
"github.copilot.chat.byok.ollamaEndpoint": "http://localhost:11434/YOUR_API_KEY"
```

---

## ğŸ› ï¸ Setup Instructions

1. Clone the repo
```bash
git clone https://github.com/your-org/fauxllama.git
cd fauxllama
```
2. Create your environment config
```bash
cp .env.example .env
```
3. Start with Docker Compose
```bash
docker-compose up --build
```

Once running, the API will be live at:
http://localhost:11434

---

## ğŸ”‘ API Key Management
To create and manage API keys:

Visit the admin panel at:
http://localhost:11434/

Login using your ADMIN_PASSWORD from .env

Create a new API key per user

Each key maps to a user and is required for accessing chat endpoints.

---

## ğŸ“¡ API Overview
All requests must include a valid API_KEY in the URL path:

```bash
POST /<API_KEY>/v1/chat/completions
```

---

## ğŸ–¥ VS Code Integration
To use fauxllama as your GitHub Copilot Chat backend:

1. Open VS Code settings.json

2. Add:

    ```json
    "github.copilot.chat.byok.ollamaEndpoint": "http://localhost:11434/YOUR_API_KEY"
    ```
3. Restart VS Code

Now, Copilot Chat will route requests through fauxllama.

---
**fauxllama** â€“ Not a real llama. Just your LLM in disguise. ğŸ¦™
